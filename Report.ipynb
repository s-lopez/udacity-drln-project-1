{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation project\n",
    "This notebook describes the implementation and illustrates the live performance of the Bananator agents in the actual environment.\n",
    "\n",
    "# Background\n",
    "#### Deep Q Networks\n",
    "The original paper on Deep Q Networks (short: DQN) can be found [here](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). An excellent in-depth introduction to Reinforcement Learning can be found in this [famous book](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) by Sutton & Barto.\n",
    "\n",
    "This repository follows the ideas proposed in the DQN paper and recreates a Deep Q Network. The network takes a state (a 37-dimensional observation of the environment) as input and outputs the state-action values of the four actions that are available to the agent (move forward and back, turn left and right). These state-action values are an estimate of the expected total reward that the agent will get from that moment on if it follows that action.\n",
    "\n",
    "Once the DQN has produced the state-action values, the agent will decide on an action following an epsilon-greedy policy, i.e. it will choose the action with the highest value with a probability of $1-\\epsilon$. With a probability of $\\epsilon$, the agent will choose an action at random. $\\epsilon$ equals one in the first episode and slowly decreases to a small number >0 with time.\n",
    "\n",
    "#### Prioritized experience replay\n",
    "Many improvements on DQN have been proposed. All of them have been combined into \"Rainbow\", an alogrithm which outperforms vanilla DQN systematically. One of the most important contributions is prioritized experience replay (short: PER). In this variation, the experiences are sampled from the replay buffer according to their TD error. Experiences that had a larger TD error are sampled with a higher probability. More details on PER can be consulted [here](https://arxiv.org/abs/1511.05952). [This blog post](https://danieltakeshi.github.io/2019/07/14/per/) was particularly useful to understand the implementation details.\n",
    "\n",
    "The agents in this repository augments DQN with PER and shows both results side by side. The vanilla agent bananator_vanilla implements DQN only, while bananator_per implements DQN and PER.\n",
    "\n",
    "# Parameters\n",
    "#### Network parameters\n",
    "The DQN has three layers. The input size is 37 and the output size 4. The middle layer have an input size of 90 and an output size of 45.\n",
    "\n",
    "#### Agent parameters\n",
    "Agent parameters are defined as a named tuple in the same file that contains the agent. This ensures consistency when importing. The parameters used can be examined below, in the section \"Load the agents\".\n",
    "\n",
    "# Results\n",
    "\n",
    "![Results](media/results.png)\n",
    "\n",
    "The vanilla agent reaches the \"solved\" threshold after about 500 episodes. Surprisingly, the PER agent does not perform as well as expected. This is not a serious problem, though, since the most likely cause is the choice of hyperparameters. With enough time, we would find a collection of values which would outperform vanilla DQN. Due to severe time constrains, I have to skip the hyperparameter search at this point.\n",
    "\n",
    "# Improvements\n",
    "\n",
    "The following are suggestions on how to improve the performance.\n",
    "* **Rewrite the replay buffer**. The replay buffer is implemented as a python deque, which performs fast queue/deque operations, but is [inefficient (O(n))](https://docs.python.org/3/library/collections.html#collections.deque) when sampling from the middle of the buffer. Having a faster replay buffer means faster trainings and more time for experiments.\n",
    "* **Implement dynamic PER parameters.** In this implementation, the PER parameters $\\alpha$ and $\\beta$ are implemented as constants. It should be easy to implement them as variables that can change as the training progresses, and according to the paper that is expected to improve performance.\n",
    "* **Perform a hyperparameter search.** There is guaranteed a better set of hyperparameters available. During development, average scores of up to 16 were seen.\n",
    "* **Implement the rest of Rainbow.** There are many other known augmentations to the original DQN model.\n",
    "* **Run the training several times.** Deep reinforcement learning is [known to depend critically on (random) initial conditions](https://www.alexirpan.com/2018/02/14/rl-hard.html). Re-train several times with the same parameters to find the best performing agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent demonstration\n",
    "Below we will see the trained agents following a greedy policy in the actual environment.\n",
    "\n",
    "**Note:** Running the cell under \"Initialization\" will open a full-screen Unity window. Switch windows or run the whole script at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# Invite our agent & import utils\n",
    "from dqn_agent import Agent, Parameters\n",
    "from random import random as rnd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "PATH_TO_ENV = \"Banana_Linux/Banana.x86\"\n",
    "BRAIN = \"BananaBrain\"\n",
    "TRAINING = False\n",
    "\n",
    "env = UnityEnvironment(file_name=PATH_TO_ENV, no_graphics=TRAINING)\n",
    "bananator = env.brains[BRAIN]\n",
    "\n",
    "# number of actions\n",
    "ACTION_SIZE = bananator.vector_action_space_size\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=TRAINING)[BRAIN]\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "STATE_SIZE = len(state)\n",
    "\n",
    "def act(env, action, brain_name=BRAIN) -> tuple:\n",
    "    \"\"\"Lets brain_name perform action on the environment env.\n",
    "    Returns a tuple of reward, next_state, done\"\"\"\n",
    "    action_result = env.step(action)[brain_name] # Act on the environment and observe the result\n",
    "    return (action_result.rewards[0], # reward from action\n",
    "            action_result.vector_observations[0], # next state\n",
    "            action_result.local_done[0]) # True if the episode ended\n",
    "    \n",
    "def reset(env, training=TRAINING, brain_name=BRAIN) -> tuple:\n",
    "    \"\"\"Syntactic sugar for resetting the unity environment\"\"\"\n",
    "    return env.reset(train_mode=training)[brain_name].vector_observations[0] \n",
    "\n",
    "def visualize(agent, env): \n",
    "    state = reset(env)\n",
    "    score = 0\n",
    "    done = False\n",
    "    j = 0\n",
    "    while not done:\n",
    "        j += 1\n",
    "        action, _ = agent.decide(state)      # Choose an action based on the state\n",
    "        reward, next_state, done = act(env, action)    # Send the action to the environment\n",
    "        score += reward                                # Update the score\n",
    "        state = next_state                             # Roll over the state to next time step\n",
    "    print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the agents' parameters\n",
    "vanilla = torch.load('models/vanilla-checkpoint.pth')\n",
    "per = torch.load('models/per-checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer_size: 100000\n",
      "batch_size: 64\n",
      "gamma: 0.99\n",
      "tau: 0.001\n",
      "lr: 0.0005\n",
      "update_every: 4\n",
      "state_size: 37\n",
      "action_size: 4\n",
      "use_per: False\n",
      "per_min_priority: 0.05\n",
      "per_prio_coeff: 1\n",
      "per_w_bias_coeff: 1\n",
      "seed: 557\n"
     ]
    }
   ],
   "source": [
    "# Prepare vanilla\n",
    "bananator_vanilla = Agent(vanilla['agent_params'], cuda=False)\n",
    "bananator_vanilla.qnetwork_local.load_state_dict(vanilla['state_dict'])\n",
    "\n",
    "# Examine vanilla's parameters -- See dqn_agent.py for a description of the individual fields\n",
    "for name, value in zip(vanilla['agent_params']._fields, vanilla['agent_params']):\n",
    "    print(f'{name}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffer_size: 100000\n",
      "batch_size: 64\n",
      "gamma: 0.99\n",
      "tau: 0.001\n",
      "lr: 0.0005\n",
      "update_every: 4\n",
      "state_size: 37\n",
      "action_size: 4\n",
      "use_per: True\n",
      "per_min_priority: 0.05\n",
      "per_prio_coeff: 0.7\n",
      "per_w_bias_coeff: 0.7\n",
      "seed: 961\n"
     ]
    }
   ],
   "source": [
    "# Prepare per\n",
    "bananator_per = Agent(per['agent_params'], cuda=False)\n",
    "bananator_per.qnetwork_local.load_state_dict(per['state_dict'])\n",
    "\n",
    "# Examine per's parameters -- See dqn_agent.py for a description of the individual fields\n",
    "for name, value in zip(per['agent_params']._fields, per['agent_params']):\n",
    "    print(f'{name}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 17.0\n"
     ]
    }
   ],
   "source": [
    "visualize(bananator_vanilla, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 8.0\n"
     ]
    }
   ],
   "source": [
    "visualize(bananator_per, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
